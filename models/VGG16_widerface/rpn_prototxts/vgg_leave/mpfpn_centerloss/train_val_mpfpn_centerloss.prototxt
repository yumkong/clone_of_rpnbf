name: "VGG_ILSVRC_16"

input: "data"
input_dim: 1
input_dim: 3
input_dim: 224
input_dim: 224

# for conv23 at stride 4
input: "labels_s4"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s4"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

##### centerloss label for conv23 at stride 4
input: "labels_s4_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s4_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
####  end center loss
input: "bbox_targets_s4"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 4  		# 4 (coords) * 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "bbox_loss_weights_s4"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 4  		# 4 (coords) * 1 anchor
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 56  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

# for conv345 at stride 8
input: "labels_s8"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 4           # 3(anchors): [8 16 32]--> 10 anchors
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s8"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 4           # 3(anchors): [8 16 32]     
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

##### centerloss label for conv23 at stride 4
input: "labels_s8_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s8_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
####  end center loss

input: "bbox_targets_s8"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 16  		# 4 (coords) * 3(anchors): [8 16 32] 
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "bbox_loss_weights_s8"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 16  		# 4 (coords) * 3(anchors): [8 16 32] 
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 28  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

# for conv3456 at stride 16
input: "labels_s16"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 3            # 4(anchors)        
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14 		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s16"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 3           # 4(anchors)        
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14 		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

##### centerloss label for conv23 at stride 4
input: "labels_s16_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "labels_weights_s16_cl"
input_dim: 1 		# to be changed on-the-fly to match input dim
input_dim: 1            # 1 anchor
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
####  end center loss

input: "bbox_targets_s16"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 12  		# 4 (coords) * 3(anchors) 
input_dim: 14 		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

input: "bbox_loss_weights_s16"
input_dim: 1  		# to be changed on-the-fly to match input dim
input_dim: 12  		# 4 (coords) * 3(anchors) 
input_dim: 14  		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)
input_dim: 14 		# size for 224 input image, to be changed on-the-fly to match input dim, 14--> 28 (resolution doubled)

layer {
	bottom: "data"
	top: "conv1_1"
	name: "conv1_1"
	type: "Convolution"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	convolution_param {
		num_output: 64
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv1_1"
	top: "conv1_1"
	name: "relu1_1"
	type: "ReLU"
}

layer {
	bottom: "conv1_1"
	top: "conv1_2"
	name: "conv1_2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 64
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv1_2"
	top: "conv1_2"
	name: "relu1_2"
	type: "ReLU"
}

layer {
	bottom: "conv1_2"
	top: "pool1"
	name: "pool1"
	type: "Pooling"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

layer {
	bottom: "pool1"
	top: "conv2_1"
	name: "conv2_1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 128
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv2_1"
	top: "conv2_1"
	name: "relu2_1"
	type: "ReLU"
}

layer {
	bottom: "conv2_1"
	top: "conv2_2"
	name: "conv2_2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 128
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv2_2"
	top: "conv2_2"
	name: "relu2_2"
	type: "ReLU"
}

layer {
	bottom: "conv2_2"
	top: "pool2"
	name: "pool2"
	type: "Pooling"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

#0205 added norm layer
layer {
  name: "conv2_norm_s4"
  type: "Normalize"
  bottom: "pool2"
  top: "conv2_norm_s4"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 12  #0205: 40-->12 
    }
    channel_shared: false
  }
}

layer {
	bottom: "pool2"
	top: "conv3_1"
	name: "conv3_1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv3_1"
	top: "conv3_1"
	name: "relu3_1"
	type: "ReLU"
}

layer {
	bottom: "conv3_1"
	top: "conv3_2"
	name: "conv3_2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv3_2"
	top: "conv3_2"
	name: "relu3_2"
	type: "ReLU"
}

layer {
	bottom: "conv3_2"
	top: "conv3_3"
	name: "conv3_3"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv3_3"
	top: "conv3_3"
	name: "relu3_3"
	type: "ReLU"
}
# ====================== stride 4 feature maps=======================
#0205 added norm layer
layer {
  name: "conv3_norm_s4"
  type: "Normalize"
  bottom: "conv3_3"
  top: "conv3_norm_s4"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0205: 40-->16 
    }
    channel_shared: false
  }
}

layer {
    bottom: "conv2_norm_s4"
    bottom: "conv3_norm_s4"
    top: "conv_proposal23_s4"
    name: "conv_proposal23_s4"
    type: "Concat"
    concat_param {
	axis: 1
    }
}
# =============================================================

# 0208: add a pooling layer to decimate resolution by 2
layer {
	bottom: "conv3_3"
	top: "pool3"
	name: "pool3"
	type: "Pooling"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

#0205 added norm layer (1/8 of the original spatial resolution)
layer {
  name: "conv3_norm_s8"
  type: "Normalize"
  bottom: "pool3"
  top: "conv3_norm_s8"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0205: 40-->16 
    }
    channel_shared: false
  }
}

layer {
	bottom: "pool3"
	top: "conv4_1"
	name: "conv4_1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv4_1"
	top: "conv4_1"
	name: "relu4_1"
	type: "ReLU"
}

layer {
	bottom: "conv4_1"
	top: "conv4_2"
	name: "conv4_2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv4_2"
	top: "conv4_2"
	name: "relu4_2"
	type: "ReLU"
}

layer {
	bottom: "conv4_2"
	top: "conv4_3"
	name: "conv4_3"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv4_3"
	top: "conv4_3"
	name: "relu4_3"
	type: "ReLU"
}

# ============== for conv4 s8====================
# dim reduce and relu
layer {
	name: "conv4_3_reduce"
	bottom: "conv4_3"
	top: "conv4_3_reduce"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
	bottom: "conv4_3_reduce"
	top: "conv4_3_reduce"
	name: "relu_conv4_3_reduce"
	type: "ReLU"
}

#0208 delete deconv layer

# normalize
layer {
  name: "conv4_norm_s8"
  type: "Normalize"
  bottom: "conv4_3_reduce"
  top: "conv4_norm_s8"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0207: 40-->16 
    }
    channel_shared: false
  }
}

#====================for conv4 s16=============================
# 0208: add a pooling layer to decimate resolution by 2
layer {
	bottom: "conv4_3"
	top: "pool4"
	name: "pool4"
	type: "Pooling"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

layer {
	bottom: "pool4"
	top: "conv5_1"
	name: "conv5_1"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv5_1"
	top: "conv5_1"
	name: "relu5_1"
	type: "ReLU"
}

layer {
	bottom: "conv5_1"
	top: "conv5_2"
	name: "conv5_2"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv5_2"
	top: "conv5_2"
	name: "relu5_2"
	type: "ReLU"
}

layer {
	bottom: "conv5_2"
	top: "conv5_3"
	name: "conv5_3"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
	type: "Convolution"
	convolution_param {
		num_output: 512
		pad: 1
		kernel_size: 3
	}
}

layer {
	bottom: "conv5_3"
	top: "conv5_3"
	name: "relu5_3"
	type: "ReLU"
}

# dim reduce and relu
layer {
	name: "conv5_3_reduce"
	bottom: "conv5_3"
	top: "conv5_3_reduce"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
	bottom: "conv5_3_reduce"
	top: "conv5_3_reduce"
	name: "relu_conv5_3_reduce"
	type: "ReLU"
}

# =============== proposal conv345 for stride 8 ======================
#0208 change from deconv 4x to 2x
layer {
  bottom: "conv5_3_reduce" 
  top: "conv5_3_2x"
  name: "conv5_3_2x" 
  type: "Deconvolution"
  convolution_param {
    kernel_size: 4 
    stride: 2
    num_output: 256
    group: 256
    pad: 1
    weight_filler: { type: "bilinear" } 
    bias_term: false
  }
  param { lr_mult: 0 decay_mult: 0 }
}

# normalize
layer {
  name: "conv5_norm_s8"
  type: "Normalize"
  bottom: "conv5_3_2x"
  top: "conv5_norm_s8"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0207: 40-->16 
    }
    channel_shared: false
  }
}

#elemtwise addition of conv3_norm, conv4_norm and conv5_norm
layer {
  name: "eltwise_sum345"
  type: "Eltwise"
  bottom: "conv3_norm_s8"
  bottom: "conv4_norm_s8"
  bottom: "conv5_norm_s8"
  top: "conv_proposal345_s8"
  eltwise_param { operation: SUM }
}
# =============== end of proposal conv345 for stride 8 ======================

#0208: remove deconv

# normalize
layer {
  name: "conv5_norm_s16"
  type: "Normalize"
  bottom: "conv5_3_reduce"
  top: "conv5_norm_s16"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0207: 40-->16 
    }
    channel_shared: false
  }
}

# -------------------- for conv6's -----------------------------
layer {
	bottom: "conv5_3"
	top: "pool5"
	name: "pool5"
	type: "Pooling"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 2
	}
}

layer {
	name: "conv6_1"
	bottom: "pool5"
	top: "conv6_1"	
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 1024
		pad: 1
		kernel_size: 3
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
	bottom: "conv6_1"
	top: "conv6_1"
	name: "relu6_1"
	type: "ReLU"
}

layer {
	name: "conv6_reduce"
	bottom: "conv6_1"
	top: "conv6_reduce"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 256
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
	bottom: "conv6_reduce"
	top: "conv6_reduce"
	name: "relu_conv6_reduce"
	type: "ReLU"
}

# deconv
layer {
  bottom: "conv6_reduce" 
  top: "conv6_2x"
  name: "conv6_2x" 
  type: "Deconvolution"
  convolution_param {
    kernel_size: 4 
    stride: 2
    num_output: 256
    group: 256
    pad: 1
    weight_filler: { type: "bilinear" } 
    bias_term: false
  }
  param { lr_mult: 0 decay_mult: 0 }
}

# normalize
layer {
  name: "conv6_norm_s16"
  type: "Normalize"
  bottom: "conv6_2x"
  top: "conv6_norm_s16"
  norm_param {
    across_spatial: false
    scale_filler {
      type: "constant"
      value: 16  #0207: 40-->16 
    }
    channel_shared: false
  }
}

#elemtwise addition of conv5_norm and conv6_norm
layer {
  name: "eltwise_sum56"
  type: "Eltwise"
  bottom: "conv5_norm_s16"
  bottom: "conv6_norm_s16"
  top: "conv_proposal56_s16"
  eltwise_param { operation: SUM }
}

#-----------------------layer for s4-------------------------
layer {
   name: "conv_proposal_s4"
   type: "Convolution"
   bottom: "conv_proposal23_s4"
   top: "conv_proposal_s4"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256  #0205: 512->256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "relu_proposal_s4"
   type: "ReLU"
   bottom: "conv_proposal_s4"
   top: "conv_proposal_s4"
}

# 0816 added for centerloss (currently use 16d)
layer {
	name: "conv_proposal_s4_16d"
	bottom: "conv_proposal_s4"
	top: "conv_proposal_s4_16d"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 16 # 10, 2,128
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}


layer {
   name: "proposal_cls_score_s4"
   type: "Convolution"
   bottom: "conv_proposal_s4_16d" #"conv_proposal_s4"
   top: "proposal_cls_score_s4"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 2   # 2(bg/fg) * 3(anchors) -->2*10
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "proposal_bbox_pred_s4"
   type: "Convolution"
   bottom: "conv_proposal_s4"
   top: "proposal_bbox_pred_s4"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 4	# 4 * 3(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

#-----------------------layer for s8-------------------------
layer {
   name: "conv_proposal_s8"
   type: "Convolution"
   bottom: "conv_proposal345_s8"
   top: "conv_proposal_s8"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256  #0205: 512->256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "relu_proposal_s8"
   type: "ReLU"
   bottom: "conv_proposal_s8"
   top: "conv_proposal_s8"
}

# 0816 added for centerloss (currently use 16d)
layer {
	name: "conv_proposal_s8_16d"
	bottom: "conv_proposal_s8"
	top: "conv_proposal_s8_16d"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 16 # 10, 2,128
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
   name: "proposal_cls_score_s8"
   type: "Convolution"
   bottom: "conv_proposal_s8_16d" #"conv_proposal_s8"
   top: "proposal_cls_score_s8"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 8   # 2(bg/fg) * 3(anchors) -->2*10
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "proposal_bbox_pred_s8"
   type: "Convolution"
   bottom: "conv_proposal_s8"
   top: "proposal_bbox_pred_s8"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 16	# 4 * 3(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

#-----------------------layer for s16-------------------------
layer {
   name: "conv_proposal_s16"
   type: "Convolution"
   bottom: "conv_proposal56_s16"
   top: "conv_proposal_s16"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 256  #0205: 512->256
	   kernel_size: 3
	   pad: 1
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "relu_proposal_s16"
   type: "ReLU"
   bottom: "conv_proposal_s16"
   top: "conv_proposal_s16"
}

# 0816 added for centerloss (currently use 16d)
layer {
	name: "conv_proposal_s16_16d"
	bottom: "conv_proposal_s16"
	top: "conv_proposal_s16_16d"
	param {
		lr_mult: 1.0
		decay_mult: 1.0
	}
	param {
		lr_mult: 2.0
		decay_mult: 0
	}
	type: "Convolution"
	convolution_param {
		num_output: 16 # 10, 2,128
		pad: 0
		kernel_size: 1
		weight_filler {
			type: "xavier"
		}
		bias_filler{
			type: "constant"
			value: 0
		}
	}
}

layer {
   name: "proposal_cls_score_s16"
   type: "Convolution"
   bottom: "conv_proposal_s16_16d" #"conv_proposal_s16"
   top: "proposal_cls_score_s16"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 6   # 2(bg/fg) * 3(anchors) -->2*10
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

layer {
   name: "proposal_bbox_pred_s16"
   type: "Convolution"
   bottom: "conv_proposal_s16"
   top: "proposal_bbox_pred_s16"
	param {
		lr_mult: 1.0
	}
	param {
		lr_mult: 2.0
	}
   convolution_param{
	   num_output: 12	# 4 * 3(anchors) 
	   kernel_size: 1
	   pad: 0
	   stride: 1
	   weight_filler {
		 type: "gaussian"
		 std: 0.01
	   }
	   bias_filler {
		 type: "constant"
		 value: 0
	   }
   }
}

#-----------------------output s4------------------------

# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score_s4"
   top: "proposal_cls_score_reshape_s4"
   name: "proposal_cls_score_reshape_s4"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_s4"
   top: "labels_reshape_s4"
   name: "labels_reshape_s4"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_weights_s4"
   top: "labels_weights_reshape_s4"
   name: "labels_weights_reshape_s4"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

################################
## num x ch x hei x wid ==> num x hei x wid x chl 
layer {  
    bottom: "conv_proposal_s4_16d" #"conv_proposal_2d_cl"  
    top: "conv_proposal_s4_permute"
    name: "conv_proposal_s4_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_s4_cl" #"labels_s4"  
    top: "labels_s4_permute"
    name: "labels_s4_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_weights_s4_cl"  #"labels_weights_s4"  
    top: "labels_weights_s4_permute"
    name: "labels_weights_s4_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}
############## 0816 rpn center loss ###############
layer {
  name: "center_loss_s4"
  type: "RpnCenterLoss"
  bottom: "conv_proposal_s4_permute" #num x height x width x chl
  bottom: "labels_s4_permute"          #num x height x width x 1
  bottom: "labels_weights_s4_permute"  #num x height x width x 1
  top: "center_loss_s4"
  param {
    lr_mult: 1
    decay_mult: 0 #2 
  }
  center_loss_param {
    num_output: 2  #two centers: face or non-face
    center_filler {
      type: "xavier"
    }
    axis: 3  #only last dimension (the original channel dim = feature length = 256)
    lr: 0.1 #default 0.05
    radius: 0.1 #default 0.1
  }
  loss_weight: 1 # 0.01
}
######################

layer {
   name: "loss_cls_s4"
   type: "SoftmaxWithLoss"
   bottom: "proposal_cls_score_reshape_s4"
   bottom: "labels_reshape_s4"
   bottom: "labels_weights_reshape_s4"
   top: "loss_cls_s4"
   loss_weight: 1
}

layer {
   name: "accuarcy_s4"
   type: "Accuracy"
   bottom: "proposal_cls_score_reshape_s4"
   bottom: "labels_reshape_s4"
   top: "accuarcy_s4"
}

layer {
  name: "loss_bbox_s4"
  type: "SmoothL1Loss"
  bottom: "proposal_bbox_pred_s4"
  bottom: "bbox_targets_s4"
  bottom: "bbox_loss_weights_s4"
  top: "loss_bbox_s4"
  loss_weight: 1  #0224: 20->200->1(0308 for nodivanchor)
  loss_param {
        normalization: VALID
  }
}

#-----------------------output s8------------------------

# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score_s8"
   top: "proposal_cls_score_reshape_s8"
   name: "proposal_cls_score_reshape_s8"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_s8"
   top: "labels_reshape_s8"
   name: "labels_reshape_s8"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_weights_s8"
   top: "labels_weights_reshape_s8"
   name: "labels_weights_reshape_s8"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

################################
## num x ch x hei x wid ==> num x hei x wid x chl 
layer {  
    bottom: "conv_proposal_s8_16d" #"conv_proposal_2d_cl"  
    top: "conv_proposal_s8_permute"
    name: "conv_proposal_s8_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_s8_cl"  #"labels_s8"
    top: "labels_s8_permute"
    name: "labels_s8_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_weights_s8_cl"  #"labels_weights_s8" 
    top: "labels_weights_s8_permute"
    name: "labels_weights_s8_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}
############## 0816 rpn center loss ###############
layer {
  name: "center_loss_s8"
  type: "RpnCenterLoss"
  bottom: "conv_proposal_s8_permute" #num x height x width x chl
  bottom: "labels_s8_permute"          #num x height x width x 1
  bottom: "labels_weights_s8_permute"  #num x height x width x 1
  top: "center_loss_s8"
  param {
    lr_mult: 1
    decay_mult: 0 #2 
  }
  center_loss_param {
    num_output: 2  #two centers: face or non-face
    center_filler {
      type: "xavier"
    }
    axis: 3  #only last dimension (the original channel dim = feature length = 256)
    lr: 0.1 #default 0.05
    radius: 0.1 #default 0.1
  }
  loss_weight: 1 # 0.01
}
######################

layer {
   name: "loss_cls_s8"
   type: "SoftmaxWithLoss"
   bottom: "proposal_cls_score_reshape_s8"
   bottom: "labels_reshape_s8"
   bottom: "labels_weights_reshape_s8"
   top: "loss_cls_s8"
   loss_weight: 1
}

layer {
   name: "accuarcy_s8"
   type: "Accuracy"
   bottom: "proposal_cls_score_reshape_s8"
   bottom: "labels_reshape_s8"
   top: "accuarcy_s8"
}

layer {
  name: "loss_bbox_s8"
  type: "SmoothL1Loss"
  bottom: "proposal_bbox_pred_s8"
  bottom: "bbox_targets_s8"
  bottom: "bbox_loss_weights_s8"
  top: "loss_bbox_s8"
  loss_weight: 1  #0224: 20->50->1(0308 for nodivanchor)
  loss_param {
        normalization: VALID
  }
}


#-----------------------output s16------------------------

# to enable the calculation of softmax loss, we first reshape blobs related to SoftmaxWithLoss
layer {
   bottom: "proposal_cls_score_s16"
   top: "proposal_cls_score_reshape_s16"
   name: "proposal_cls_score_reshape_s16"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 2
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_s16"
   top: "labels_reshape_s16"
   name: "labels_reshape_s16"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

layer {
   bottom: "labels_weights_s16"
   top: "labels_weights_reshape_s16"
   name: "labels_weights_reshape_s16"
   type: "Reshape"
   reshape_param{
	   shape {
			dim: 0 
			dim: 1
			dim: -1 
			dim: 0
		}
	}
}

################################
## num x ch x hei x wid ==> num x hei x wid x chl 
layer {  
    bottom: "conv_proposal_s16_16d" #"conv_proposal_2d_cl"  
    top: "conv_proposal_s16_permute"
    name: "conv_proposal_s16_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_s16_cl"  #"labels_s16"  
    top: "labels_s16_permute"
    name: "labels_s16_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}

layer {  
    bottom: "labels_weights_s16_cl"  #"labels_weights_s16"  
    top: "labels_weights_s16_permute"
    name: "labels_weights_s16_permute"  
    type: "Permute"  
    permute_param {    
      order: 0 
      order: 2  
      order: 3
      order: 1  
    }
}
############## 0816 rpn center loss ###############
layer {
  name: "center_loss_s16"
  type: "RpnCenterLoss"
  bottom: "conv_proposal_s16_permute" #num x height x width x chl
  bottom: "labels_s16_permute"          #num x height x width x 1
  bottom: "labels_weights_s16_permute"  #num x height x width x 1
  top: "center_loss_s16"
  param {
    lr_mult: 1
    decay_mult: 0 #2 
  }
  center_loss_param {
    num_output: 2  #two centers: face or non-face
    center_filler {
      type: "xavier"
    }
    axis: 3  #only last dimension (the original channel dim = feature length = 256)
    lr: 0.1 #default 0.05
    radius: 0.1 #default 0.1
  }
  loss_weight: 1 # 0.01
}
######################

layer {
   name: "loss_cls_s16"
   type: "SoftmaxWithLoss"
   bottom: "proposal_cls_score_reshape_s16"
   bottom: "labels_reshape_s16"
   bottom: "labels_weights_reshape_s16"
   top: "loss_cls_s16"
   loss_weight: 1
}

layer {
   name: "accuarcy_s16"
   type: "Accuracy"
   bottom: "proposal_cls_score_reshape_s16"
   bottom: "labels_reshape_s16"
   top: "accuarcy_s16"
}

layer {
  name: "loss_bbox_s16"
  type: "SmoothL1Loss"
  bottom: "proposal_bbox_pred_s16"
  bottom: "bbox_targets_s16"
  bottom: "bbox_loss_weights_s16"
  top: "loss_bbox_s16"
  loss_weight: 1  #0224: 10->12->1(0308 for nodivanchor)
  loss_param {
        normalization: VALID
  }
}
